# File names which should be inputs to this benchmark.
# Unix-style globbing is supported.
input:
    path:
        - 'runs/*/*/*/*cholesky_native*.out'
        - 'runs/*/*/*/*det_native*.out'
        - 'runs/*/*/*/*dot_native*.out'
        - 'runs/*/*/*/*inv_native*.out'
        - 'runs/*/*/*/*eig_native*.out'
        - 'runs/*/*/*/*lu_native*.out'
        - 'runs/*/*/*/*qr_native*.out'
        - 'runs/*/*/*/*svd_native*.out'
    # Format of input files
    format: csv

# Aggregation method (e.g. min, median, max, mean)
aggregation: min

# Axis and series column names
axis:
- Function
- Size

series:
- Prefix

# Create another table (or Excel filter) for each value in these columns
variants:
- Arch
- Mode

# Are higher values better?
higher-is-better: false

# Value columns
values:
- Time

# Precompute columns using lambda functions
precomputed:
    Function: "row['Function'].capitalize()"
    Mode: "'Serial' if row['File'].split('.')[1].split('_')[-1] == 'seq' else 'Parallel'"
    Arch: "(row['Directory'].split('/')[-3].split('_')[-3:-2]+['Unknown'])[0]"
    Implementation: "'numpy' if row['Function'] in ('Dot', 'Det', 'Inv') else 'scipy'"

# Filter out: require columns to NOT be certain values (after filter-keep)
#filter-out:
#    Prefix: [Prefix]

# Number format in Python str.format() style, or the number of digits of
# precision to show in numbers in pretty-printed pivot tables
number-format: 2

# Do we figure out the number of decimals only once, using the max value,
# and disregard smaller precision numbers?
number-format-max-only: false

