
# File names which should be inputs to this benchmark.
# Unix-style globbing is supported.
input:
    path: 'runs/*/*/*/*_ibench*.out'
    # Format of input files
    format: csv
    filter:
        '^@': drop
        '':

# Aggregation method (e.g. min, median, max, mean)
aggregation: min

# Axis and series column names
axis:
- Function
- Size

series:
- Prefix

# Create another table (or Excel filter) for each value in these columns
variants:
- Arch
- Mode
  # TODO Size requires us to grab params from launcher

# Are higher values better?
higher-is-better: false

# Value columns
values:
- Time

# Precompute columns using lambda functions
precomputed:
    Mode: "'Serial' if row['File'].split('.')[1].split('_')[2] == 'seq' else 'Parallel'"
    Arch: "(row['Directory'].split('/')[-3].split('_')[-3:-2]+['Unknown'])[0]"
    Prefix: "(lambda p: 'Native-C' if p == 'native' else p)(row['Directory'].split('/')[-1])"
    Implementation: "'numpy' if row['Function'] in ('Dot', 'Det', 'Inv') else 'scipy'"
    #Size: "int(row['origSize'])"

# Filter: require certain values for columns (after precompute and rename)
filter-in:

# Filter out: require columns to NOT be certain values (after filter-keep)
filter-out:
#    Prefix: [Prefix]

# Number format in Python str.format() style, or the number of digits of
# precision to show in numbers in pretty-printed pivot tables
number-format: 2

# Do we figure out the number of decimals only once, using the max value,
# and disregard smaller precision numbers?
number-format-max-only: false

